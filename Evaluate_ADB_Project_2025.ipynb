{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj9-3U--Krvc"
      },
      "source": [
        "# ADB Phase 2 Project Evaluation Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV2Nc_f8Mbqh"
      },
      "source": [
        "**Purpose**: This notebook evaluates the performance of a semantic search project by analyzing databases of various sizes.\n",
        "\n",
        "### Evaluation Focus:\n",
        "- **Database Sizes**:\n",
        "  - 1 Million Records\n",
        "  - 10 Million Records\n",
        "  - 20 Million Records\n",
        "\n",
        "For each database size, this notebook will:\n",
        "- Download the database\n",
        "- Use the `VecDB` class (implemented by students) to retrieve queries\n",
        "- Evaluate and report retrieval time, accuracy, and RAM usage.\n",
        "\n",
        "### Project Constraints:\n",
        "Refer to the project document for details on RAM, Disk, Time, and Score constraints.\n",
        "\n",
        "### Notebook Structure:\n",
        "1. **Part 1 - Modifiable Cells**:\n",
        "   - Includes cells that teams are allowed to modify, specifically for these variables only:\n",
        "     - GitHub repository link (including PAT token).\n",
        "     - Google Drive IDs for indexes files.\n",
        "     - Paths for loading existing indexes.\n",
        "\n",
        "2. **Part 2 - Non-Modifiable Cells**:\n",
        "   - Contains essential setup and evaluation code that must not be modified.\n",
        "   - Students should only modify inputs in Part 1 to ensure smooth execution of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4EV_xB6Kw17"
      },
      "source": [
        "## Part 1 - Modifiable Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODP-iztLtBV"
      },
      "source": [
        "Each team must provide a unique GitHub repository link that includes a PAT token. This link will allow the notebook to download the necessary code for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCR6Z8ABxE3w",
        "outputId": "cc2f61de-c13f-4049-8763-e4a2ce814729"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'vec_db'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Amira764/vec_db.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7QSIX510KMF"
      },
      "source": [
        "# Database Path Instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsUXWYom6xRv"
      },
      "source": [
        "Teams need to specify paths for each database (1M, 10M, 20M records) as follows:\n",
        "\n",
        "1. Zip each database directory/file after generation.\n",
        "2. Upload the zip file to Google Drive.\n",
        "3. Share the file with \"Anyone with the link.\"\n",
        "4. Extract the file ID from the link (e.g., for `https://drive.google.com/file/d/1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah/view`, the ID is `1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah`).\n",
        "5. Assign each ID to the appropriate variable in Part 1.\n",
        "6. Provide the local PATH for each database to be passed to the initializer for automatic loading of the database and index (to be submitted during the project final phase). (This path could be folder name or whatever string you need).\n",
        "\n",
        "**Note**: The code will download and unzip these files automatically. Once extracted, the local path for each database should be specified to enable the notebook to load databases and indexes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kK46_ZVe5L3u"
      },
      "outputs": [],
      "source": [
        "TEAM_NUMBER = 5\n",
        "GDRIVE_ID_DB_1M = \"1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\"\n",
        "GDRIVE_ID_DB_10M = \"1Jho9rair77eWdA5-iI_qDT2spJNesuDe\"\n",
        "GDRIVE_ID_DB_20M = \"1a7KL0BmPeW8SsckllNTtCX42L1gS_8U0\"\n",
        "PATH_DB_1M = \"saved_index_1m\"\n",
        "PATH_DB_10M = \"saved_index_10m\"\n",
        "PATH_DB_20M = \"saved_index_20m\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGLg01fsujm"
      },
      "source": [
        "**Seed Number**:\n",
        "This number will be changed during discussions by the instructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G44iH6jnObEj"
      },
      "outputs": [],
      "source": [
        "SEED_NUMBER = 10\n",
        "import random\n",
        "random.seed(SEED_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWaZ-ByWOIcK"
      },
      "source": [
        "**Final Submission Checklist**:\n",
        "Ensure the following items are included in your final submission:\n",
        "- `TEAM_NUMBER`\n",
        "- GitHub clone link (with PAT token)\n",
        "- Google Drive IDs for each database:\n",
        "  - `GDRIVE_ID_DB_1M`, `GDRIVE_ID_DB_10M`, `GDRIVE_ID_DB_20M`\n",
        "- Paths for each database:\n",
        "  - `PATH_DB_1M`, `PATH_DB_10M`, `PATH_DB_20M`\n",
        "- Project document detailing the work and findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzFTOecwu8wj"
      },
      "source": [
        "## Part 2: Do Not Modify Beyond This Point\n",
        "### Note:\n",
        "This section contains setup and evaluation code that should not be edited by students. Only the instructor may modify this section in case of a major bug.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "67NUn3KWXA6u"
      },
      "outputs": [],
      "source": [
        "# This code is not working now for some reason on Colab\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqujj7tYTA1l",
        "outputId": "57a04b15-72b7-40df-dd32-997bd59df674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\menna\\Desktop\\Fall Senior II\\ADB\\Project\\vec_db\\vec_db\n"
          ]
        }
      ],
      "source": [
        "%cd vec_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJmXzFdisD7P"
      },
      "source": [
        "This cell to run any additional requirement that your code need <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaPjq2hMqd20",
        "outputId": "4b697845-7be9-424e-9847-50043b4e1c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\menna\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: memory-profiler in c:\\users\\menna\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.61.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\menna\\anaconda3\\lib\\site-packages (from memory-profiler->-r requirements.txt (line 2)) (5.9.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
            "     -------------------------- ----------- 30.7/44.0 kB 660.6 kB/s eta 0:00:01\n",
            "     ----------------------------------- -- 41.0/44.0 kB 495.5 kB/s eta 0:00:01\n",
            "     -------------------------------------- 44.0/44.0 kB 359.1 kB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-1.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: Pillow in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\menna\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
            "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: networkx in c:\\users\\menna\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\menna\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\menna\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\menna\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.2)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: anyio in c:\\users\\menna\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\menna\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\menna\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.14.0)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\menna\\anaconda3\\lib\\site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.1.7)\n",
            "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
            "   ---------------------------------------- 0.0/488.0 kB ? eta -:--:--\n",
            "   ----- ---------------------------------- 61.4/488.0 kB 3.2 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 122.9/488.0 kB 1.4 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 204.8/488.0 kB 1.8 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 307.2/488.0 kB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  481.3/488.0 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 488.0/488.0 kB 2.0 MB/s eta 0:00:00\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.1/12.0 MB 2.9 MB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   - -------------------------------------- 0.5/12.0 MB 3.1 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.6/12.0 MB 3.3 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/12.0 MB 3.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/12.0 MB 3.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/12.0 MB 3.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/12.0 MB 3.4 MB/s eta 0:00:04\n",
            "   -- ------------------------------------- 0.7/12.0 MB 3.4 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.3/12.0 MB 2.9 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.4/12.0 MB 3.0 MB/s eta 0:00:04\n",
            "   ----- ---------------------------------- 1.6/12.0 MB 3.0 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 1.9/12.0 MB 3.1 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 2.0/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.2/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.3/12.0 MB 3.2 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 2.4/12.0 MB 2.5 MB/s eta 0:00:04\n",
            "   --------- ------------------------------ 2.9/12.0 MB 2.9 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 3.0/12.0 MB 2.9 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 3.2/12.0 MB 2.9 MB/s eta 0:00:04\n",
            "   ----------- ---------------------------- 3.4/12.0 MB 3.0 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 3.6/12.0 MB 3.0 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 3.7/12.0 MB 3.0 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 3.9/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 4.1/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 4.2/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 4.4/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 4.6/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   --------------- ------------------------ 4.7/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 4.9/12.0 MB 3.1 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 5.1/12.0 MB 3.2 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 5.1/12.0 MB 3.2 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 5.1/12.0 MB 3.2 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 5.1/12.0 MB 3.2 MB/s eta 0:00:03\n",
            "   ------------------- -------------------- 5.7/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 5.9/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.0/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.2/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 6.3/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 6.5/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 6.7/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 6.9/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 7.0/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 7.2/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 7.3/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 7.4/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.6/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 7.8/12.0 MB 3.2 MB/s eta 0:00:02\n",
            "   -------------------------- ------------- 8.0/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.1/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 8.2/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.4/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 8.6/12.0 MB 3.3 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 8.8/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 8.9/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 9.1/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 9.2/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 9.4/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 9.5/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.7/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 9.9/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.0/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.2/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 10.3/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 10.5/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.6/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.8/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.0 MB 3.6 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.1/12.0 MB 3.5 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.3/12.0 MB 3.5 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.4/12.0 MB 3.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.6/12.0 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.7/12.0 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.9/12.0 MB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.0/12.0 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.0/12.0 MB 3.3 MB/s eta 0:00:00\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
            "   -------- ------------------------------- 122.9/566.1 kB 3.6 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 276.5/566.1 kB 3.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 440.3/566.1 kB 3.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 566.1/566.1 kB 3.2 MB/s eta 0:00:00\n",
            "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
            "   ---------------------------------------- 0.0/341.4 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 143.4/341.4 kB 4.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 286.7/341.4 kB 3.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 341.4/341.4 kB 3.6 MB/s eta 0:00:00\n",
            "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.2/2.7 MB 3.3 MB/s eta 0:00:01\n",
            "   ---- ----------------------------------- 0.3/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 0.5/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.6/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.7/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   ------------ --------------------------- 0.9/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 1.0/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 1.0/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 1.0/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.5/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.6/2.7 MB 3.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.7/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 1.9/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.0/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 2.1/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 2.3/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.4/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.6/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  2.7/2.7 MB 3.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.7/2.7 MB 3.0 MB/s eta 0:00:00\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.36.0 safetensors-0.7.0 sentence-transformers-5.1.2 tokenizers-0.22.1 transformers-4.57.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install memory-profiler >> log.txt\n",
        "# !pip install -r requirements.txt\n",
        "# !pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0DALR498__"
      },
      "source": [
        "This cell to download the zip files and unzip them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSv2z0PVp6HA",
        "outputId": "f9cf0dbd-2c18-4042-8471-dab82ff0cb93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14Gp_C3LxLuYIyF-zL5q-xFXKQ8KOFtZp\n",
            "To: /content/sematic_search_DB/saved_db_100k.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 41.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XbEP6sU0k0UbuPcQLkHJP7cdPtebpsbD\n",
            "To: /content/sematic_search_DB/saved_db_1m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 37.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DX0tw9YDlvRthjMq3LQ6_aUyp3BvTC1r\n",
            "To: /content/sematic_search_DB/saved_db_5m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 91.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jho9rair77eWdA5-iI_qDT2spJNesuDe\n",
            "To: /content/sematic_search_DB/saved_db_10m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 34.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sPIgNxIuNDUUBnTvAuPMLryd1mqmgwV3\n",
            "To: /content/sematic_search_DB/saved_db_15m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 116MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j1gAU3kvdRqcOoKI5K5FgMMUZpOQANah\n",
            "To: /content/sematic_search_DB/saved_db_20m.zip\n",
            "100% 28.5M/28.5M [00:00<00:00, 142MB/s]\n",
            "Archive:  saved_db_100k.zip\n",
            "  inflating: saved_db_100k.csv       \n",
            "Archive:  saved_db_1m.zip\n",
            "  inflating: saved_db_1m.csv         \n",
            "Archive:  saved_db_5m.zip\n",
            "  inflating: saved_db_5m.csv         \n",
            "Archive:  saved_db_10m.zip\n",
            "  inflating: saved_db_10m.csv        \n",
            "Archive:  saved_db_15m.zip\n",
            "  inflating: saved_db_15m.csv        \n",
            "Archive:  saved_db_20m.zip\n",
            "  inflating: saved_db_20m.csv        \n"
          ]
        }
      ],
      "source": [
        "!gdown $GDRIVE_ID_DB_1M -O saved_db_1m.zip\n",
        "!gdown $GDRIVE_ID_DB_10M -O saved_db_10m.zip\n",
        "!gdown $GDRIVE_ID_DB_20M -O saved_db_20m.zip\n",
        "!unzip saved_db_1m.zip\n",
        "!unzip saved_db_10m.zip\n",
        "!unzip saved_db_20m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp19a7NgJtmg"
      },
      "source": [
        "Download and Generate The DBs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "usqLi0C8K1h6"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zfAitTOIJtQD"
      },
      "outputs": [],
      "source": [
        "PATH_DB_VECTORS_20M = \"OpenSubtitles_en_20M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_10M = \"OpenSubtitles_en_10M_emb_64.dat\"\n",
        "PATH_DB_VECTORS_1M = \"OpenSubtitles_en_1M_emb_64.dat\"\n",
        "if not os.path.exists(PATH_DB_VECTORS_20M):\n",
        "    !gdown \"1a7KL0BmPeW8SsckllNTtCX42L1gS_8U0\" -O \"OpenSubtitles_en_20M_emb_64.dat\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2SPhELotKXaO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DIMENSION = 64\n",
        "def create_other_DB_size(input_file, output_file, target_rows, embedding_dim = DIMENSION):\n",
        "    # Configuration\n",
        "    dtype = 'float32'\n",
        "\n",
        "    # 1. Determine the shape of the source file\n",
        "    # We calculate rows based on file size to be safe, or you can hardcode 20_000_000\n",
        "    file_size_bytes = os.path.getsize(input_file)\n",
        "    itemsize = np.dtype(dtype).itemsize\n",
        "    total_rows = file_size_bytes // (embedding_dim * itemsize)\n",
        "\n",
        "    print(f\"Source detected: {total_rows} rows.\")\n",
        "\n",
        "    # 2. Open source in read mode ('r')\n",
        "    # This uses almost 0 RAM, it just points to the file on disk\n",
        "    source_memmap = np.memmap(\n",
        "        input_file,\n",
        "        dtype=dtype,\n",
        "        mode='r',\n",
        "        shape=(total_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 3. Create the new file in write mode ('w+')\n",
        "    # We define the shape as the target size (1M, 64)\n",
        "    dest_memmap = np.memmap(\n",
        "        output_file,\n",
        "        dtype=dtype,\n",
        "        mode='w+',\n",
        "        shape=(target_rows, embedding_dim)\n",
        "    )\n",
        "\n",
        "    # 4. Copy the data\n",
        "    # This transfers the binary blocks directly\n",
        "    print(\"Copying data...\")\n",
        "    dest_memmap[:] = source_memmap[:target_rows]\n",
        "\n",
        "    # 5. Flush to save changes to disk\n",
        "    dest_memmap.flush()\n",
        "\n",
        "    print(f\"Success! Saved first {target_rows} rows to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_1uvNlMELYk0"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(PATH_DB_VECTORS_1M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_1M, 1_000_000)\n",
        "if not os.path.exists(PATH_DB_VECTORS_10M):\n",
        "    create_other_DB_size(PATH_DB_VECTORS_20M, PATH_DB_VECTORS_10M, 10_000_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4GDcog3dMZY"
      },
      "source": [
        "Code to generate the queries that will be used to evaluate the questions.\n",
        "\n",
        "Note: English sentences will be changed at submission day\n",
        "\n",
        "The first sentence will be used just for warmup, then the others will be used for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xIPNJ3nWPErt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\menna\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "queries_embed_file = \"queries_emb_64.dat\"\n",
        "if not os.path.exists(queries_embed_file):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    batch_sentences = [\n",
        "        \"Hello World\",\n",
        "        \"We are Software Engineering Students\",\n",
        "        \"What's the best way to be a good human?\",\n",
        "        \"What a good day\"\n",
        "    ]\n",
        "    model = SentenceTransformer('minishlab/potion-base-2M')\n",
        "    queries_np = model.encode(batch_sentences, convert_to_numpy=True)\n",
        "    queries_np.tofile(queries_embed_file)\n",
        "else:\n",
        "    queries_np = np.fromfile(queries_embed_file, dtype=np.float32).reshape(-1, DIMENSION)\n",
        "\n",
        "query_dummy = queries_np[0].reshape(1, DIMENSION)\n",
        "queries = [queries_np[1].reshape(1, DIMENSION), queries_np[2].reshape(1, DIMENSION), queries_np[3].reshape(1, DIMENSION)]\n",
        "queries_np = queries_np[1:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi9E370vaMGM"
      },
      "source": [
        "Generate the sorted_ids for each DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ad_K9Zz9bZ8m"
      },
      "outputs": [],
      "source": [
        "actual_sorted_ids_file = \"actual_sorted_ids_20m.dat\"\n",
        "saved_top_k = 30_000\n",
        "needed_top_k = 10_000\n",
        "if not os.path.exists(actual_sorted_ids_file):\n",
        "    vectors = np.memmap(PATH_DB_VECTORS_20M, dtype='float32', mode='r', shape=(20_000_000, DIMENSION))\n",
        "    actual_sorted_ids_20m = np.argsort(np.dot(vectors, queries_np.T) / (1e-45 + np.linalg.norm(vectors, axis=1)[:, None] * np.linalg.norm(queries_np, axis=1)), axis=0)[-saved_top_k:][::-1].T\n",
        "    actual_sorted_ids_20m.tofile(actual_sorted_ids_file)\n",
        "else:\n",
        "    actual_sorted_ids_20m = np.fromfile(actual_sorted_ids_file, dtype=np.int32).reshape(-1, saved_top_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShuPR-gGlX3f"
      },
      "source": [
        "These are the functions for running and reporting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Sg2vfYgeyavn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "import gc\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "def run_queries(db, queries, top_k, actual_ids, num_runs):\n",
        "    \"\"\"\n",
        "    Run queries on the database and record results for each query.\n",
        "\n",
        "    Parameters:\n",
        "    - db: Database instance to run queries on.\n",
        "    - queries: List of query vectors.\n",
        "    - top_k: Number of top results to retrieve.\n",
        "    - actual_ids: List of actual results to evaluate accuracy.\n",
        "    - num_runs: Number of query executions to perform for testing.\n",
        "\n",
        "    Returns:\n",
        "    - List of Result\n",
        "    \"\"\"\n",
        "    global results\n",
        "    results = []\n",
        "    for i in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrieve(queries[i], top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids[i]))\n",
        "    return results\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    \"\"\"\n",
        "    Run queries and measure memory usage during the execution.\n",
        "\n",
        "    Parameters:\n",
        "    - args: Arguments to be passed to the run_queries function.\n",
        "\n",
        "    Returns:\n",
        "    - results: The results of the run_queries.\n",
        "    - memory_diff: The difference in memory usage before and after running the queries.\n",
        "    \"\"\"\n",
        "    global results\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    \"\"\"\n",
        "    Evaluate the results based on accuracy and runtime.\n",
        "    Scores are negative. So getting 0 is the best score.\n",
        "\n",
        "    Parameters:\n",
        "    - results: A list of Result objects\n",
        "\n",
        "    Returns:\n",
        "    - avg_score: The average score across all queries.\n",
        "    - avg_runtime: The average runtime for all queries.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k, out_len = 10_000):\n",
        "    \"\"\"\n",
        "    Retrieve the IDs from the sorted list of actual IDs.\n",
        "    actual IDs has the top_k for the 20 M database but for other databases we have to remove the numbers higher than the max size of the DB.\n",
        "\n",
        "    Parameters:\n",
        "    - actual_sorted_ids: A list of lists containing the sorted actual IDs for each query.\n",
        "    - k: The DB size.\n",
        "\n",
        "    Returns:\n",
        "    - List of lists containing the actual IDs for each query for this DB.\n",
        "    \"\"\"\n",
        "    return [[id for id in actual_sorted_ids_one_q if id < k] for actual_sorted_ids_one_q in actual_sorted_ids][:out_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrOlipAOmy9K"
      },
      "source": [
        "This code to actually run the class you have been implemented. The `VecDB` class should take the database path, and index path that you provided.<br>\n",
        "Note at the submission I'll not run the insert records. <br>\n",
        "The query istelf will be changed at submissions day but not the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Yi34qMLHsqCn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\menna\\anaconda3\\Lib\\tracemalloc.py:560: size=296 B (+296 B), count=2 (+2), average=148 B\n",
            "c:\\Users\\menna\\anaconda3\\Lib\\tracemalloc.py:423: size=296 B (+296 B), count=2 (+2), average=148 B\n",
            "c:\\Users\\menna\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3517: size=296 B (+0 B), count=1 (+0), average=296 B\n",
            "c:\\Users\\menna\\anaconda3\\Lib\\codeop.py:126: size=286 B (+0 B), count=2 (+0), average=143 B\n",
            "c:\\Users\\menna\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: size=160 B (+0 B), count=1 (+0), average=160 B\n"
          ]
        }
      ],
      "source": [
        "# check memory usage for the import line independently\n",
        "import tracemalloc\n",
        "tracemalloc.start()\n",
        "start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "end_snapshot = tracemalloc.take_snapshot()\n",
        "stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "for stat in stats[:5]:  # show top differences\n",
        "    print(stat)\n",
        "\n",
        "tracemalloc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "g-hCaQNqlBP0"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "to_print_arr = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-UFbhBPlQtz",
        "outputId": "55072ace-347c-4b15-a359-38d46ca41295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 5\n",
            "****************************************\n",
            "Evaluating DB of size 20M\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "database_info = {\n",
        "    # \"1M\": {\n",
        "    #     \"database_file_path\": PATH_DB_VECTORS_1M,\n",
        "    #     \"index_file_path\": PATH_DB_1M,\n",
        "    #     \"size\": 10**6\n",
        "    # },\n",
        "    # \"10M\": {\n",
        "    #     \"database_file_path\": PATH_DB_VECTORS_10M,\n",
        "    #     \"index_file_path\": PATH_DB_10M,\n",
        "    #     \"size\": 10 * 10**6\n",
        "    # },\n",
        "    \"20M\": {\n",
        "        \"database_file_path\": PATH_DB_VECTORS_20M,\n",
        "        \"index_file_path\": PATH_DB_20M,\n",
        "        \"size\": 20 * 10**6\n",
        "    }\n",
        "}\n",
        "\n",
        "from vec_db import VecDB\n",
        "\n",
        "for db_name, info in database_info.items():\n",
        "    print(f\"*\"*40)\n",
        "    print(f\"Evaluating DB of size {db_name}\")\n",
        "\n",
        "    # This part added to check RAM usage for the class init function\n",
        "    tracemalloc.start()\n",
        "    start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "    db = VecDB(database_file_path = info[\"database_file_path\"], index_file_path = info[\"index_file_path\"], new_db = False)\n",
        "\n",
        "    end_snapshot = tracemalloc.take_snapshot()\n",
        "    stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "    for stat in stats[:5]:  # show top differences\n",
        "        print(stat)\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, info[\"size\"], needed_top_k)\n",
        "    # Make a dummy run query to make everything fresh and loaded (wrap up)\n",
        "    # CRITICAL DON'T CACHE ANYTHING IN THE QUERY FUNCTION\n",
        "\n",
        "    # This part added to check RAM usage for the run queries with another method\n",
        "    tracemalloc.start()\n",
        "    start_snapshot = tracemalloc.take_snapshot()\n",
        "\n",
        "    res = run_queries(db, query_dummy, 5, actual_ids, 1)\n",
        "\n",
        "    end_snapshot = tracemalloc.take_snapshot()\n",
        "    stats = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
        "    for stat in stats[:5]:  # show top differences\n",
        "        print(stat)\n",
        "    tracemalloc.stop()\n",
        "    # actual runs to evaluate\n",
        "    res, mem = memory_usage_run_queries((db, queries, 5, actual_ids, 3))\n",
        "    eval = evaluate_result(res)\n",
        "    to_print = f\"{db_name}\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "    print(to_print)\n",
        "    to_print_arr.append(to_print)\n",
        "    del db\n",
        "    del actual_ids\n",
        "    del res\n",
        "    del mem\n",
        "    del eval\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt1_7ihfB37Z",
        "outputId": "5f6a1b32-5ca3-4e3c-c314-e61caaec99ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Team Number 5\n",
            "1M\tscore\t0.0\ttime\t0.01\tRAM\t0.00 MB\n",
            "10M\tscore\t0.0\ttime\t0.02\tRAM\t0.00 MB\n"
          ]
        }
      ],
      "source": [
        "print(\"Team Number\", TEAM_NUMBER)\n",
        "print(\"\\n\".join(to_print_arr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXdMoPfVsUgK"
      },
      "outputs": [],
      "source": [
        "!git log"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
